{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project03_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC81qZsE898J"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "9xj0iv6c9Tf7",
        "outputId": "0e67e8bc-9187-4fb9-e57c-29a98689a128"
      },
      "source": [
        "import pandas as pd \r\n",
        "traindata = pd.read_csv(\"train.csv\",  encoding='utf8' )\r\n",
        "del traindata['Unnamed: 0']\r\n",
        "traindata.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>547530</td>\n",
              "      <td>Beşiktaş'ın eski teknik direktörü Slaven Bilic...</td>\n",
              "      <td>spor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42683</td>\n",
              "      <td>14 Şubat, Katolik Kilisesi’nin azizlerinden St...</td>\n",
              "      <td>yazarlar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>770925</td>\n",
              "      <td>İstanbul Kağıthane’de gece yarısı sokakta oyun...</td>\n",
              "      <td>video</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>473224</td>\n",
              "      <td>Ziraat Türkiye Kupası maçında deplasmanda Amed...</td>\n",
              "      <td>video</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44833</td>\n",
              "      <td>Eski Brezilyalı yıldız oyuncu Ronaldo, Brezily...</td>\n",
              "      <td>spor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                                               text     label\n",
              "0  547530  Beşiktaş'ın eski teknik direktörü Slaven Bilic...      spor\n",
              "1   42683  14 Şubat, Katolik Kilisesi’nin azizlerinden St...  yazarlar\n",
              "2  770925  İstanbul Kağıthane’de gece yarısı sokakta oyun...     video\n",
              "3  473224  Ziraat Türkiye Kupası maçında deplasmanda Amed...     video\n",
              "4   44833  Eski Brezilyalı yıldız oyuncu Ronaldo, Brezily...      spor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deSw7FVBWzz1"
      },
      "source": [
        "testdata = pd.read_csv(\"test.csv\",  encoding='utf8' )\r\n",
        "del testdata['Unnamed: 0']\r\n",
        "\r\n",
        "X_train = traindata['text']\r\n",
        "y_train = traindata['label']\r\n",
        "\r\n",
        "X_test = testdata['text']\r\n",
        "y_test = testdata['label']\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHU4SI7Y_5uW"
      },
      "source": [
        "from pickle import load\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers.merge import concatenate\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "# calculating the maximum document length\r\n",
        "def max_length(lines):\r\n",
        "\treturn max([len(s.split()) for s in lines])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZBLfmK1FZIi"
      },
      "source": [
        "from sklearn import preprocessing\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "def encode_labels(y_train, y_test):\r\n",
        "\tle = LabelEncoder()\r\n",
        "\tle.fit(y_train)\r\n",
        "\ty_train_enc = le.transform(y_train)\r\n",
        "\ty_test_enc = le.transform(y_test)\r\n",
        "\treturn y_train_enc, y_test_enc\r\n",
        "\r\n",
        "y_train_enc, y_test_enc = encode_labels(y_train, y_test)\r\n",
        "y_train_encoded_labels = np_utils.to_categorical(y_train_enc)\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_encoded_labels, test_size=0.2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwhVzZd_DoLN"
      },
      "source": [
        "**Run up until this part then run whichever model you want**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKDX_lVEuO5"
      },
      "source": [
        "Randomized Weights Static CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usYtCWB5qXO6",
        "outputId": "dbc583ae-f90d-4181-c51a-4c228da22988"
      },
      "source": [
        "\r\n",
        "# define the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "  inputs1 = Input(shape=(length,))\r\n",
        "  embedding1 = Embedding(vocab_size, 100, trainable = False)(inputs1)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  flat1 = Flatten()(pool1)\r\n",
        "  \r\n",
        "  dense1 = Dense(10, activation='relu')(flat1)\r\n",
        "  outputs = Dense(5, activation='softmax')(dense1)\r\n",
        "  model = Model(inputs=[inputs1], outputs=outputs)\r\n",
        "  \r\n",
        "  # summarize\r\n",
        "  print(model.summary())\r\n",
        "  plot_model(model, show_shapes=True)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# calculating max document length\r\n",
        "length = max_length(X_train)\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "X_train_tokens = tokenizer.fit_on_texts(X_train)\r\n",
        "encoded = tokenizer.texts_to_sequences(X_train)\r\n",
        "encoded2 = tokenizer.texts_to_sequences(X_val)\r\n",
        "\r\n",
        "X_train = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "X_val = pad_sequences(encoded2, maxlen=length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "# calculating vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "\r\n",
        "# defining model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fitting model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=5)\r\n",
        "\r\n",
        "\r\n",
        "y_pred=np.argmax(model.predict(X_val), axis=-1) \r\n",
        "print(\"Classification Report:\")\r\n",
        "print(classification_report(y_val.argmax(axis=1),y_pred))\r\n",
        "\r\n",
        "print(\"confusion matrix:\")\r\n",
        "print(confusion_matrix(y_val.argmax(axis=1),y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 6994\n",
            "Vocabulary size: 180206\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 6994)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 6994, 100)         18020600  \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 6991, 32)          12832     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6991, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 3495, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 111840)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1118410   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 19,151,897\n",
            "Trainable params: 1,131,297\n",
            "Non-trainable params: 18,020,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 7s 32ms/step - loss: 1.6019 - accuracy: 0.2248 - val_loss: 1.4666 - val_accuracy: 0.3519\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 1.4274 - accuracy: 0.3339 - val_loss: 1.3885 - val_accuracy: 0.4406\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 1.3650 - accuracy: 0.4368 - val_loss: 1.3405 - val_accuracy: 0.4444\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 1.3146 - accuracy: 0.4474 - val_loss: 1.3030 - val_accuracy: 0.4594\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 1.2841 - accuracy: 0.4545 - val_loss: 1.2629 - val_accuracy: 0.4775\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.42      0.37       322\n",
            "           1       0.28      0.20      0.23       306\n",
            "           2       1.00      0.00      0.01       337\n",
            "           3       0.52      0.88      0.65       310\n",
            "           4       0.65      0.90      0.75       325\n",
            "\n",
            "    accuracy                           0.48      1600\n",
            "   macro avg       0.56      0.48      0.40      1600\n",
            "weighted avg       0.56      0.48      0.40      1600\n",
            "\n",
            "confusion matrix:\n",
            "[[134  50   0  98  40]\n",
            " [102  61   0  87  56]\n",
            " [126  76   1  70  64]\n",
            " [ 34   2   0 274   0]\n",
            " [  4  27   0   0 294]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axDnEEkGFyZ-"
      },
      "source": [
        "Randomized Weights NonStatic CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTNullZ0E9jO",
        "outputId": "ceb4cbb3-076b-4b8e-df5a-01702ca0d71e"
      },
      "source": [
        "# define the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "  inputs1 = Input(shape=(length,))\r\n",
        "  embedding1 = Embedding(vocab_size, 100, trainable = True)(inputs1)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        " \r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  flat1 = Flatten()(pool1)\r\n",
        "  \r\n",
        "  dense1 = Dense(10, activation='relu')(flat1)\r\n",
        "  outputs = Dense(5, activation='softmax')(dense1)\r\n",
        "  model = Model(inputs=[inputs1], outputs=outputs)\r\n",
        "  \r\n",
        "  # summarize\r\n",
        "  print(model.summary())\r\n",
        "  plot_model(model, show_shapes=True)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# calculating max document length\r\n",
        "length = max_length(X_train)\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "X_train_tokens = tokenizer.fit_on_texts(X_train)\r\n",
        "encoded = tokenizer.texts_to_sequences(X_train)\r\n",
        "encoded2 = tokenizer.texts_to_sequences(X_val)\r\n",
        "\r\n",
        "X_train = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "X_val = pad_sequences(encoded2, maxlen=length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "# calculating vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "\r\n",
        "# defining model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fitting model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=5)\r\n",
        "\r\n",
        "\r\n",
        "y_pred=np.argmax(model.predict(X_val), axis=-1) \r\n",
        "print(\"Classification Report:\")\r\n",
        "print(classification_report(y_val.argmax(axis=1),y_pred))\r\n",
        "\r\n",
        "print(\"confusion matrix:\")\r\n",
        "print(confusion_matrix(y_val.argmax(axis=1),y_pred))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 4107\n",
            "Vocabulary size: 181399\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4107)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 4107, 100)         18139900  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 4104, 32)          12832     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4104, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 2052, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 65664)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                656650    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 18,809,437\n",
            "Trainable params: 18,809,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 42s 199ms/step - loss: 1.3388 - accuracy: 0.4362 - val_loss: 0.8985 - val_accuracy: 0.6712\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 39s 196ms/step - loss: 0.5832 - accuracy: 0.7859 - val_loss: 0.5931 - val_accuracy: 0.7987\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 39s 197ms/step - loss: 0.1434 - accuracy: 0.9575 - val_loss: 0.6458 - val_accuracy: 0.7738\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 40s 198ms/step - loss: 0.0333 - accuracy: 0.9936 - val_loss: 0.7533 - val_accuracy: 0.7581\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 40s 198ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 0.7376 - val_accuracy: 0.7781\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.74      0.72       344\n",
            "           1       0.93      0.89      0.91       307\n",
            "           2       0.74      0.62      0.67       328\n",
            "           3       0.72      0.76      0.74       316\n",
            "           4       0.82      0.90      0.86       305\n",
            "\n",
            "    accuracy                           0.78      1600\n",
            "   macro avg       0.78      0.78      0.78      1600\n",
            "weighted avg       0.78      0.78      0.78      1600\n",
            "\n",
            "confusion matrix:\n",
            "[[255   2  21  42  24]\n",
            " [  1 272  17  16   1]\n",
            " [ 46   7 203  37  35]\n",
            " [ 39   9  29 239   0]\n",
            " [ 23   2   4   0 276]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KF1xnnYD02L"
      },
      "source": [
        "**Run the following two parts for pretrained models then run whichever model you want**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F17wfritxMk6",
        "outputId": "f723c2af-78d3-42f5-c97e-3a5c85ffd9d4"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import gensim.models.keyedvectors as word2vec\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "pretrained_project2 = Word2Vec.load('/content/drive/MyDrive/word2vec/word2vecmodel.model')\r\n",
        "\r\n",
        "trmodel = word2vec.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/trmodel', binary= True)\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIAfmdZ5xRdi",
        "outputId": "6dfeb4c1-a753-43e2-dd20-f470371a32bd"
      },
      "source": [
        "from gensim import models\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "tokenizer = Tokenizer()\r\n",
        "\r\n",
        "X_train_tokens = tokenizer.fit_on_texts(X_train)\r\n",
        "encoded = tokenizer.texts_to_sequences(X_train)\r\n",
        "encoded2 = tokenizer.texts_to_sequences(X_val)\r\n",
        "\r\n",
        "def get_weight_matrix(embedding, vocab, embedding_size):\r\n",
        "  vocab_size = len(vocab) + 1\r\n",
        "  weight_matrix = np.zeros((vocab_size, embedding_size))\r\n",
        "  for word, i in vocab.items():\r\n",
        "    try:\r\n",
        "      weight_matrix[i] = embedding.wv.word_vec(word)\r\n",
        "    except KeyError:\r\n",
        "      weight_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_size)\r\n",
        "        \r\n",
        "  return weight_matrix\r\n",
        "\r\n",
        "embeddingmatrix =get_weight_matrix(pretrained_project2,tokenizer.word_index, 500) #embedding matrix for project2 word2vec\r\n",
        "\r\n",
        "embeddingmatrix2 =get_weight_matrix(trmodel,tokenizer.word_index, 400) #embedding matrix for trmodel word2vec"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIpParktGHRZ"
      },
      "source": [
        "CNN with Pretrained Word Embeddings from Project02 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6PFDVlfxUWR",
        "outputId": "c6499694-e11a-4354-e423-7913c6784cfb"
      },
      "source": [
        "# defining the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "\r\n",
        "  inputs1 = Input(shape=(length,))\r\n",
        "  embedding1 = Embedding(vocab_size, 500, weights= [embeddingmatrix])(inputs1)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  flat1 = Flatten()(pool1)\r\n",
        "  \r\n",
        "  dense1 = Dense(10, activation='relu')(flat1)\r\n",
        "  outputs = Dense(5, activation='softmax')(dense1)\r\n",
        "  model = Model(inputs=[inputs1], outputs=outputs)\r\n",
        "  \r\n",
        "  print(model.summary())\r\n",
        "  plot_model(model, show_shapes=True)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# calculating max document length\r\n",
        "length = max_length(X_train)\r\n",
        "\r\n",
        "\r\n",
        "X_train = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "X_val = pad_sequences(encoded2, maxlen=length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "# calculating vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "\r\n",
        "# define model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fit model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=5)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "y_pred=np.argmax(model.predict(X_val), axis=-1) \r\n",
        "print(\"Classification Report:\")\r\n",
        "print(classification_report(y_val.argmax(axis=1),y_pred))\r\n",
        "\r\n",
        "print(\"confusion matrix:\")\r\n",
        "print(confusion_matrix(y_val.argmax(axis=1),y_pred))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 6994\n",
            "Vocabulary size: 181732\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 6994)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 6994, 500)         90866000  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 6991, 32)          64032     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 6991, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 3495, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111840)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1118410   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 92,048,497\n",
            "Trainable params: 92,048,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 210s 1s/step - loss: 1.3790 - accuracy: 0.3967 - val_loss: 1.0461 - val_accuracy: 0.5606\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 203s 1s/step - loss: 0.8010 - accuracy: 0.6818 - val_loss: 0.6964 - val_accuracy: 0.7319\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 204s 1s/step - loss: 0.3107 - accuracy: 0.8958 - val_loss: 0.5479 - val_accuracy: 0.8056\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 205s 1s/step - loss: 0.1195 - accuracy: 0.9677 - val_loss: 0.6017 - val_accuracy: 0.7975\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 204s 1s/step - loss: 0.0596 - accuracy: 0.9838 - val_loss: 0.5877 - val_accuracy: 0.8112\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.73      0.79       332\n",
            "           1       0.90      0.92      0.91       337\n",
            "           2       0.67      0.71      0.69       316\n",
            "           3       0.71      0.83      0.77       309\n",
            "           4       0.95      0.87      0.91       306\n",
            "\n",
            "    accuracy                           0.81      1600\n",
            "   macro avg       0.82      0.81      0.81      1600\n",
            "weighted avg       0.82      0.81      0.81      1600\n",
            "\n",
            "confusion matrix:\n",
            "[[243   0  47  39   3]\n",
            " [  1 310  11  10   5]\n",
            " [ 20  13 224  54   5]\n",
            " [ 11   8  34 256   0]\n",
            " [  6  15  20   0 265]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivo17nT7GXv5"
      },
      "source": [
        "CNN with Pretrained Word Embeddings from Turkish Word2Vec Model (Static)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F3Umz-xaQQN",
        "outputId": "64f1af7a-9c96-4899-d231-2cf157662977"
      },
      "source": [
        "# calculate the maximum document length\r\n",
        "def max_length(lines):\r\n",
        "\treturn max([len(s.split()) for s in lines])\r\n",
        "\r\n",
        "\r\n",
        "# define the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "  # channel 1\r\n",
        "  inputs1 = Input(shape=(length,))\r\n",
        "  embedding1 = Embedding(vocab_size, 400, weights= [embeddingmatrix2], trainable = False)(inputs1)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  flat1 = Flatten()(pool1)\r\n",
        "  \r\n",
        "  dense1 = Dense(10, activation='relu')(flat1)\r\n",
        "  outputs = Dense(5, activation='softmax')(dense1)\r\n",
        "  model = Model(inputs=[inputs1], outputs=outputs)\r\n",
        "  \r\n",
        "  # summarize\r\n",
        "  print(model.summary())\r\n",
        "  plot_model(model, show_shapes=True)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# calculate max document length\r\n",
        "length = max_length(X_train)\r\n",
        "\r\n",
        "\r\n",
        "X_train = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "X_val = pad_sequences(encoded2, maxlen=length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "# calculate vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "\r\n",
        "# define model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fit model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=5)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "y_pred=np.argmax(model.predict(X_val), axis=-1) \r\n",
        "print(\"Classification Report:\")\r\n",
        "print(classification_report(y_val.argmax(axis=1),y_pred))\r\n",
        "\r\n",
        "print(\"confusion matrix:\")\r\n",
        "print(confusion_matrix(y_val.argmax(axis=1),y_pred))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 4107\n",
            "Vocabulary size: 180232\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4107)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 4107, 400)         72092800  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 4104, 32)          51232     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4104, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 2052, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 65664)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                656650    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 72,800,737\n",
            "Trainable params: 707,937\n",
            "Non-trainable params: 72,092,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 17s 74ms/step - loss: 1.2658 - accuracy: 0.4823 - val_loss: 0.8999 - val_accuracy: 0.6162\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 15s 73ms/step - loss: 0.8243 - accuracy: 0.6623 - val_loss: 0.7933 - val_accuracy: 0.7075\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 14s 72ms/step - loss: 0.6176 - accuracy: 0.7358 - val_loss: 0.7852 - val_accuracy: 0.6844\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 14s 73ms/step - loss: 0.4640 - accuracy: 0.8036 - val_loss: 0.6734 - val_accuracy: 0.7750\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 14s 72ms/step - loss: 0.3791 - accuracy: 0.8635 - val_loss: 0.6129 - val_accuracy: 0.7837\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.64      0.72       320\n",
            "           1       0.89      0.93      0.91       311\n",
            "           2       0.56      0.67      0.61       321\n",
            "           3       0.79      0.80      0.79       316\n",
            "           4       0.91      0.87      0.89       332\n",
            "\n",
            "    accuracy                           0.78      1600\n",
            "   macro avg       0.79      0.78      0.79      1600\n",
            "weighted avg       0.79      0.78      0.79      1600\n",
            "\n",
            "confusion matrix:\n",
            "[[206   4  87  16   7]\n",
            " [  0 290  10   8   3]\n",
            " [ 35  10 216  42  18]\n",
            " [  5  12  47 252   0]\n",
            " [  8  11  23   0 290]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ZLJLMpv1uA"
      },
      "source": [
        "CNN with Pretrained Word Embeddings from Turkish Word2Vec Model (Non Static)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tABuCLiTcHFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d784156-748d-46bd-900d-c69d4b2ada3b"
      },
      "source": [
        "# calculate the maximum document length\r\n",
        "def max_length(lines):\r\n",
        "\treturn max([len(s.split()) for s in lines])\r\n",
        "\r\n",
        "\r\n",
        "# define the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "  # channel 1\r\n",
        "  inputs1 = Input(shape=(length,))\r\n",
        "  embedding1 = Embedding(vocab_size, 400, weights= [embeddingmatrix2], trainable = True)(inputs1)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  flat1 = Flatten()(pool1)\r\n",
        "  \r\n",
        "  dense1 = Dense(10, activation='relu')(flat1)\r\n",
        "  outputs = Dense(5, activation='softmax')(dense1)\r\n",
        "  model = Model(inputs=[inputs1], outputs=outputs)\r\n",
        "  \r\n",
        "  # summarize\r\n",
        "  print(model.summary())\r\n",
        "  plot_model(model, show_shapes=True)\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# calculate max document length\r\n",
        "length = max_length(X_train)\r\n",
        "\r\n",
        "\r\n",
        "X_train = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "X_val = pad_sequences(encoded2, maxlen=length, padding='post')\r\n",
        "\r\n",
        "\r\n",
        "# calculate vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "\r\n",
        "# define model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fit model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X_train, y_train , validation_data=(X_val, y_val), epochs=5)\r\n",
        "\r\n",
        "\r\n",
        "y_pred=np.argmax(model.predict(X_val), axis=-1) \r\n",
        "print(\"Classification Report:\")\r\n",
        "print(classification_report(y_val.argmax(axis=1),y_pred))\r\n",
        "\r\n",
        "print(\"confusion matrix:\")\r\n",
        "print(confusion_matrix(y_val.argmax(axis=1),y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 6994\n",
            "Vocabulary size: 180888\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 6994)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 6994, 400)         72355200  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 6991, 32)          51232     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6991, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 3495, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 111840)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1118410   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 73,524,897\n",
            "Trainable params: 73,524,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 162s 806ms/step - loss: 1.3023 - accuracy: 0.4739 - val_loss: 0.9167 - val_accuracy: 0.6637\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 161s 808ms/step - loss: 0.6025 - accuracy: 0.7958 - val_loss: 0.6258 - val_accuracy: 0.7956\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 163s 814ms/step - loss: 0.3567 - accuracy: 0.8772 - val_loss: 0.6339 - val_accuracy: 0.7881\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 165s 825ms/step - loss: 0.2145 - accuracy: 0.9374 - val_loss: 0.6612 - val_accuracy: 0.7925\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 164s 821ms/step - loss: 0.1330 - accuracy: 0.9603 - val_loss: 0.7591 - val_accuracy: 0.7994\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.84      0.74       304\n",
            "           1       0.90      0.94      0.92       332\n",
            "           2       0.74      0.59      0.66       350\n",
            "           3       0.84      0.71      0.77       306\n",
            "           4       0.88      0.94      0.91       308\n",
            "\n",
            "    accuracy                           0.80      1600\n",
            "   macro avg       0.80      0.80      0.80      1600\n",
            "weighted avg       0.80      0.80      0.80      1600\n",
            "\n",
            "confusion matrix:\n",
            "[[254   5  18   8  19]\n",
            " [ 11 312   2   4   3]\n",
            " [ 86  10 207  28  19]\n",
            " [ 21  17  52 216   0]\n",
            " [ 13   4   1   0 290]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp8gmad1EJ4R"
      },
      "source": [
        "For each model trial the code must be run from the beggining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wPT0hDmEROT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}